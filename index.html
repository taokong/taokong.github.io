<html><head>
<title>Tao Kong</title>

<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>

<style type="text/css">
 @import url("http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic");


	body
	{
	font-family:"Roboto",Helvetica,Arial,sans-serif;font-size:16px;line-height:1.5;font-weight:300;
    	background-color : #CDCDCD;
	}
    	.content
	{
    		width : 900px;
    		padding : 25px 30px;
    		margin : 25px auto;
    		background-color : #fff;
    		box-shadow: 0px 0px 10px #999;
    		border-radius: 15px; 
	}	
	table
	{
		padding: 5px;
	}
	
	table.pub_table,td.pub_td1,td.pub_td2
	{
		padding: 8px;
		width: 850px;
        border-collapse: separate;
        border-spacing: 15px;
        margin-top: -5px;
	}

	td.pub_td1
	{
		width:50px;
	}
    td.pub_td1 img
    {
        height:120px;
        width: 160px;
    }
	
	div#container
	{
		margin-left: auto;
		margin-right: auto;
		width: 820px;
		text-align: left;
		position: relative;
		background-color: #FFF;
	}
	div#DocInfo
	{
		color: #1367a7;
		height: 158px;
		margin-left: 20px;
	}
	h4,h3,h2,h1
	{
		color: #3B3B3B;
	}
	h2
	{
		font-size:130%;
	}
	p
	{
		color: #5B5B5B;
		margin-bottom: 50px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
	}
	#header_img
	{
		position: absolute;
		top: 0px; right: 0px;
    }
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}

    #mit_logo {
        position: absolute;
        left: 646px;
        top: 14px;
        width: 200px;
        height: 20px;
    }
   
    table.pub_table tr {
        outline: thin dotted #666666;
    }
    .papericon {
        border-radius: 8px; 
        -moz-box-shadow: 3px 3px 6px #888;
        -webkit-box-shadow: 3px 3px 6px #888;
        box-shadow: 3px 3px 6px #888;
        width: 180px;
	margin-top:5px;
	margin-left:5px;
	margin-bottom:5px;
    }

     .papericon_blank {

        width: 160px;
	margin-top:5px;
	margin-left:5px;
	margin-bottom:5px;
    }

    .media {
	outline: thin dotted #666666;
 	margin-bottom: 15px;	
	margin-left:10px;
    }
    .media-body {
	margin-top:5px;
	padding-left:20px;
    }
    .instructorphoto img {
	  width: 170px;
	  border-radius: 170px;
	  margin-bottom: 10px;
	}


.papers-selected h5, .papers-selected h4 { display : none; }
.papers-selected .publication { display : none; }
.paperhi-only { display : none; }
.papers-selected .paperhi { display : flex; }
.papers-selected .paperlo { display : none; }

.hidden>div {
	display:none;
}

.visible>div {
	display:block;
}
</style>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23931362-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-23931362-2');
</script>

<script type="text/javascript">
    var myPix = new Array("profile.jpg")
    function choosePic() {
        var randomNum = Math.floor(Math.random() * myPix.length);
        document.getElementById("myPicture").src = myPix[randomNum];
    };
</script>

<script>
$(document).ready(function() {
  $('.paperlo button').click(function() {
     $('.papers-container').addClass('papers-selected');
  });
  $('.paperhi button').click(function() {
     $('.papers-container').removeClass('papers-selected');
  });


	$('.text_container').addClass("hidden");

	$('.text_container').click(function() {
		var $this = $(this);

		if ($this.hasClass("hidden")) {
			$(this).removeClass("hidden").addClass("visible");
			$(this).removeClass("papericon");
		} else {
			$(this).removeClass("visible").addClass("hidden");
		}
	});


});
</script>

</head>

<body>
<div class="content">
	<div id="container">

	<table>
	<tbody><tr>
	<td><div class="instructorphoto"><img id="myPicture" src="profile.jpg" style="float:left;"></div></td>
	<script>choosePic();</script>
	<td>
	<div id="DocInfo">
		<h1>Tao Kong</h1><br>
        XXX<br><br>
        Email: <a href="mailto:taokongcn@gmail.com">taokongcn@gmail.com</a><br><br>
        <a href="https://scholar.google.com/citations?user=kSUXLPkAAAAJ&hl=en">Google Scholar</a> &bull; 
		<a href="https://arxiv.org/find/cs/1/au:+Kong_Tao/0/1/0/all/0/1">arXiv</a> &bull; 
		<a href="https://github.com/taokong">Github</a> &bull; 
		<a href="https://www.zhihu.com/people/kong-tao-72">Zhihu</a><br>
	</div><br>
    <!--
    <div id="mit_logo">
        <a href="http://www.mit.edu"><img src="image/mit.gif" height="170px" class="papericon" /></a>
    </div>
    -->
	</td>
	</tr>
	</tbody></table>
	<br>
	

<h2>Intro</h2>
<ul>
<li>​​Forging the next phase​, 2025 - Now.</li>
<li>Between 2019 and 2025, I served as a Director of Robotics Research at ByteDance, where I established and led a great robotics research team, 
	spearheading the development of cutting-edge robotic technologies and systems. 
    I received my Ph.D. from Tsinghua University in 2019, advised by <a href="https://scholar.google.com/citations?user=DbviELoAAAAJ&hl=en">Fuchun Sun</a>. 
    I visited the University of Pennsylvania, working with <a href="https://www.cis.upenn.edu/~jshi">Jianbo Shi</a>. 
	My research lies in the field of robot learning and computer vision, 
	with a particular emphasis on devising scalable, AI powered algorithms and systems that enable robots to perceive and act in the real world.
	<li><font color="#800000">I am recruiting full-time researchers and engineers in robotics.</font> 
	If you are interested in these positions, please drop me an email.</a></li>

</ul> 

<h2>Selected Projects (<a href="https://scholar.google.com/citations?hl=en&user=kSUXLPkAAAAJ&view_op=list_works&sortby=pubdate">15k+ Citations</a>)</h2>
<ul> 
      <li>
      <b>GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation</b><br>
	     <font size=3px>
		Robotics Research Team, ByteDance Research<br>
	     	<i>Tech report, 2024 </i><br>
		     [<a href="https://gr2-manipulation.github.io">Project</a>][<a href="https://arxiv.org/pdf/2410.06158">Paper</a>][<a href="https://www.youtube.com/watch?v=FHH0r_QUmyo">Video</a>][<a href="https://mp.weixin.qq.com/s/h-69PKoCkPtj4_sq9589Tw">News</a>][<a href="https://gr1-manipulation.github.io/">GR-1</a>][<a href="https://arxiv.org/pdf/2408.14368">GR-MG</a>][<a href="https://arxiv.org/abs/2308.03624">Visual-Force Learning</a>][<a href="https://arxiv.org/abs/2308.03620">Findings</a>]<br>
	     </font>
	</li>

	<li>
      <b>Vision-Language Foundation Models as Effective Robot Imitators</b><br>
	     <font size=3px>
		     Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, Tao Kong<br>
		    <b><font size=2px><font color="#800000"><i>Providing an Open-Source Robotics Learning Framework Based on VLM that Enables the Learning of a Wide Variety of Robot Skills.</i></font></font></b> <br>
	     	     <i>ICLR, 2024 </i> <br>
		     [<a href="https://roboflamingo.github.io">Project</a>][<a href="https://arxiv.org/pdf/2311.01378">Paper</a>][<a href="https://github.com/RoboFlamingo/RoboFlamingo">Code</a>][<a href="https://mp.weixin.qq.com/s/rJ5nuV4Og_2BWJbLncHOiw">News</a>][<a href="https://robovlms.github.io">RoboVLMs</a>][<a href="https://arxiv.org/abs/2307.02469">VLM study</a>]<br>
	     </font>
	</li>
	
	<li>
      <b>Navigating to Objects in Unseen Environments by Distance Prediction</b><br>
             <font size=3px>
		     Minzhao Zhu, Binglei Zhao, Tao Kong<br>
		     <b><i><font color="#800000"><font size=2px>Our base method to win the Habitat ObjectNav Challenge 2022.</font></font></i></b> <br>
      		    <i>IROS, 2022 (Oral) </i> <br>
		     [<a href="https://arxiv.org/abs/2202.03735">Paper</a>][<a href="https://mp.weixin.qq.com/s/WxA9Y3U_eJSjqQfH1iXqHA">News</a>][<a href="https://arxiv.org/abs/2303.10936">Active Perception</a>]<br>
	     </font>
       </li>
	
	<li>
      <b>iBOT: Image BERT Pre-Training with Online Tokenizer</b><br>
	     <font size=3px>
		     Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille and Tao Kong<br>
		     <b><font size=2px><font color="#800000"><i>Among the Most Influential ICLR Papers in Google Scholar Metrics 2023</i></font></font></b> <br>
		     <i>ICLR, 2022 </i> <br>
		     [<a href="https://arxiv.org/abs/2111.07832">Paper</a>][<a href="https://github.com/bytedance/ibot">Code</a>][<a href="https://mp.weixin.qq.com/s/TH7UazOVIhlk33d0Kvsgvg">News</a>][<a href="https://arxiv.org/abs/2209.03917">dBOT</a>][<a href="https://arxiv.org/abs/2110.07402">TWIST</a>]<br>
		</font> 
      </li>

      <li>
      <b>SOLO: Segmenting Objects by Locations</b><br>
	     <font size=3px>
		     Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang and Lei Li<br>
		     <b><font size=2px><font color="#800000"><i>Among the Most Influential ECCV Papers in Google Scholar Metrics 2022/2023</i></font></font></b> <br>
		     <i>ECCV, 2020 </i> <br>
		     [<a href="projects/solo">Project</a>][<a href="https://arxiv.org/abs/1912.04488">Paper</a>][<a href="https://arxiv.org/abs/2003.10152">SOLOv2</a>][<a href="https://arxiv.org/abs/2106.15947">Final</a>][<a href="https://github.com/WXinlong/SOLO">Code</a>]<br>
	      </font>
      </li>
	
      <li>
      <b>FoveaBox: Beyond Anchor-based Object Detector</b><br>
	     <font size=3px>
		     Tao Kong, Fuchun Sun, Huaping Liu, Yuning Jiang, Lei Li, Jianbo Shi<br>
		     <b><font size=2px><font color="#800000"><i>ESI Highly Cited Paper (Top 1%). </i></font></font></b> <br>
	      	     <b><font size=2px><font color="#800000"><i>Among the Most Influential TIP Papers in Google Scholar Metrics 2023 </i></font></font></b> <br>
	     	     <i>TIP, 2020 </i> <br>
		     [<a href="projects/FoveaBox">Project</a>][<a href="https://arxiv.org/abs/1904.03797">Paper</a>][<a href="https://github.com/taokong/FoveaBox">Code</a>][<a href="https://arxiv.org/abs/1901.06563">Loss</a>][<a href="https://arxiv.org/abs/1604.00600">HyperNet</a>]<br>
	     </font>
	</li>
	
</ul>


		
<h2>Honors & Awards</h2>
<ul> 
    <li>
    IROS 2024 New Generation Star Program 
    </li>
    <li>
	ICRA 2024 Co-manipulation Workshop Best Paper Finalists
    </li>
    <li>
    WAIC-Yunfan Award 2024
    </li>
    <li>
    CAA First Prize of the Natural Science Award 2023
    </li>
    <li>
    Habitat ObjectNav Challenge Winner Award 2022
    </li>
    <li>
    <a href="papers/CAAI-Tao.pdf">CAAI Excellent Doctoral Dissertation Nomination Award</a>, 2020
    </li>
    <li>
    IROS Robotic Grasping and Manipulation Competition Winner Award 2016
    </li>
    <li>
    The CCF Outstanding Undergraduate Award, 2013
    </li>
    <li>
    University Young Science Award, 2013
    </li>
    <li>
    National Scholarship, 2012/2013
    </li>
</ul>
	
<!-- 
<h2>Alumni</h2>
<ul> 
    <li>
    Feng Wang, intern at ByteDance, Tsinghua
    </li>
    <li>
    Jinghao Zhou, intern at ByteDance, CMU
    </li>
    <li>
    Yunfei Li, intern at ByteDance, Tsinghua
    </li>
    <li>
    Yiming Li, intern at ByteDance, CAS -> EPFL
    </li>
    <li>
    Ya Jing, intern at ByteDance, CAS -> ByteDance
    </li>
    <li>
    Ruihang Chu, intern at ByteDance, CUHK
    </li>
    <li>
    Yukang Chen, intern at ByteDance, CUHK
    </li>
    <li>
    Rufeng Zhang, intern at ByteDance, Tongji -> Baidu
    </li>
    <li>
    Mingxuan Jing, intern at ByteDance, Tsinghua -> CAS
    </li>
    <li>
    Xiaojian Ma, intern at ByteDance, Tsinghua -> UCLA
    </li>
    <li>
    Xinlong Wang, intern at ByteDance, Adelaide -> BAAI
    </li>
</ul> -->


<!-- <h2>Academic Service</h2>
<ul>    
    <li>
    Conference Reviewer: CVPR, ICCV, ECCV, NeurIPS, ICLR, etc.
    </li>
    <li>
    Journal Reviewer: TPAMI, IJCV, TIP, PR, etc.
    </li>
</ul> -->

<p></p>
<p align="left"><i>Last update: June, 2025</i> </p>
</body></html>
